{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to OpenResponses Python","text":"<p>The Un-official Python SDK for the Open Responses standard.</p> <p><code>openresponses-python</code> provides typed Pydantic models, a flexible client SDK, and utilities for building compliant providers.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Strict Typing: Standardized models for <code>MessageItem</code>, <code>ReasoningItem</code>, <code>ToolCallItem</code>, etc.</li> <li>Async &amp; Sync Support: Built on <code>httpx</code> for high performance.</li> <li>Semantic Streaming: First-class support for streaming reasoning and text deltas.</li> <li>Provider Agnostic: Proxy examples for OpenRouter, OpenAI, HuggingFace, Ollama, and more.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install openresponses-python\n</code></pre> <p>!!! tip \"For Development\" We recommend using uv for dependency management.</p> <pre><code>```bash\nmake install\n```\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Check out the Quickstart guid.</li> <li>Browse the API Reference.</li> <li>See ready-to-use Examples.</li> </ul>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#client","title":"Client","text":"<p>options: show_root_heading: true</p> <p>options: show_root_heading: true</p>"},{"location":"api/#openresponses.client.OpenResponsesClient","title":"<code>openresponses.client.OpenResponsesClient</code>","text":"<p>Async/Sync Client for Open Responses API.</p> Source code in <code>src/openresponses/client.py</code> <pre><code>class OpenResponsesClient:\n    \"\"\"\n    Async/Sync Client for Open Responses API.\n    \"\"\"\n    def __init__(self, base_url: str, api_key: Optional[str] = None):\n        self.base_url = base_url.rstrip(\"/\")\n        self.headers = {\"Content-Type\": \"application/json\"}\n        if api_key:\n            self.headers[\"Authorization\"] = f\"Bearer {api_key}\"\n\n    def create(\n        self,\n        model: str,\n        input: Union[str, List[MessageItem]],\n        stream: bool = False,\n        max_tool_calls: Optional[int] = None\n    ) -&gt; Union[OpenResponsesOutput, Generator[StreamEvent, None, None]]:\n        \"\"\"\n        Synchronous request to create a response.\n        \"\"\"\n        request = OpenResponsesRequest(\n            model=model,\n            input=input,\n            stream=stream,\n            max_tool_calls=max_tool_calls\n        )\n\n        url = f\"{self.base_url}/v1/responses\"\n\n        if stream:\n            return self._stream_request(url, request)\n        else:\n            with httpx.Client() as client:\n                resp = client.post(url, json=request.model_dump(), headers=self.headers, timeout=60.0)\n                resp.raise_for_status()\n                return OpenResponsesOutput(**resp.json())\n\n    def _stream_request(self, url: str, request: OpenResponsesRequest) -&gt; Generator[StreamEvent, None, None]:\n        with httpx.Client() as client:\n            with client.stream(\"POST\", url, json=request.model_dump(), headers=self.headers, timeout=60.0) as resp:\n                resp.raise_for_status()\n                for line in resp.iter_lines():\n                    if line.startswith(\"event:\"):\n                        event_type = line.split(\": \", 1)[1]\n                    elif line.startswith(\"data:\"):\n                        data_str = line.split(\": \", 1)[1]\n                        data = json.loads(data_str)\n                        yield StreamEvent(event=event_type, data=data)\n</code></pre>"},{"location":"api/#openresponses.client.OpenResponsesClient.create","title":"<code>create(model, input, stream=False, max_tool_calls=None)</code>","text":"<p>Synchronous request to create a response.</p> Source code in <code>src/openresponses/client.py</code> <pre><code>def create(\n    self,\n    model: str,\n    input: Union[str, List[MessageItem]],\n    stream: bool = False,\n    max_tool_calls: Optional[int] = None\n) -&gt; Union[OpenResponsesOutput, Generator[StreamEvent, None, None]]:\n    \"\"\"\n    Synchronous request to create a response.\n    \"\"\"\n    request = OpenResponsesRequest(\n        model=model,\n        input=input,\n        stream=stream,\n        max_tool_calls=max_tool_calls\n    )\n\n    url = f\"{self.base_url}/v1/responses\"\n\n    if stream:\n        return self._stream_request(url, request)\n    else:\n        with httpx.Client() as client:\n            resp = client.post(url, json=request.model_dump(), headers=self.headers, timeout=60.0)\n            resp.raise_for_status()\n            return OpenResponsesOutput(**resp.json())\n</code></pre>"},{"location":"api/#openresponses.client.AsyncOpenResponsesClient","title":"<code>openresponses.client.AsyncOpenResponsesClient</code>","text":"<p>Async Client for Open Responses API.</p> Source code in <code>src/openresponses/client.py</code> <pre><code>class AsyncOpenResponsesClient:\n    \"\"\"\n    Async Client for Open Responses API.\n    \"\"\"\n    def __init__(self, base_url: str, api_key: Optional[str] = None):\n        self.base_url = base_url.rstrip(\"/\")\n        self.headers = {\"Content-Type\": \"application/json\"}\n        if api_key:\n            self.headers[\"Authorization\"] = f\"Bearer {api_key}\"\n\n    async def create(\n        self,\n        model: str,\n        input: Union[str, List[MessageItem]],\n        stream: bool = False,\n        max_tool_calls: Optional[int] = None\n    ) -&gt; Union[OpenResponsesOutput, AsyncGenerator[StreamEvent, None]]:\n        request = OpenResponsesRequest(\n            model=model,\n            input=input,\n            stream=stream,\n            max_tool_calls=max_tool_calls\n        )\n\n        url = f\"{self.base_url}/v1/responses\"\n\n        if stream:\n            return self._stream_request(url, request)\n        else:\n            async with httpx.AsyncClient() as client:\n                resp = await client.post(url, json=request.model_dump(), headers=self.headers, timeout=60.0)\n                resp.raise_for_status()\n                return OpenResponsesOutput(**resp.json())\n\n    async def _stream_request(self, url: str, request: OpenResponsesRequest) -&gt; AsyncGenerator[StreamEvent, None]:\n        async with httpx.AsyncClient() as client:\n            async with client.stream(\"POST\", url, json=request.model_dump(), headers=self.headers, timeout=60.0) as resp:\n                resp.raise_for_status()\n                async for line in resp.aiter_lines():\n                    if line.startswith(\"event:\"):\n                        event_type = line.split(\": \", 1)[1]\n                    elif line.startswith(\"data:\"):\n                        data_str = line.split(\": \", 1)[1]\n                        # Handle potential keeping of newlines or empty data\n                        try:\n                            data = json.loads(data_str)\n                            yield StreamEvent(event=event_type, data=data)\n                        except json.JSONDecodeError:\n                            continue\n</code></pre>"},{"location":"api/#models","title":"Models","text":"<p>options: members: - model - input - stream - max_tool_calls</p>"},{"location":"api/#openresponses.models.OpenResponsesRequest","title":"<code>openresponses.models.OpenResponsesRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Standard Request Body for Open Responses API.</p> Source code in <code>src/openresponses/models.py</code> <pre><code>class OpenResponsesRequest(BaseModel):\n    \"\"\"Standard Request Body for Open Responses API.\"\"\"\n    model: str\n    input: Union[str, List[MessageItem]] # Can be simple text or structured items\n    stream: bool = False\n    max_tool_calls: Optional[int] = Field(default=None, description=\"Limit for provider-managed loops\")\n</code></pre>"},{"location":"api/#openresponses.models.OpenResponsesOutput","title":"<code>openresponses.models.OpenResponsesOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Standard Response Body for Open Responses API.</p> Source code in <code>src/openresponses/models.py</code> <pre><code>class OpenResponsesOutput(BaseModel):\n    \"\"\"Standard Response Body for Open Responses API.\"\"\"\n    id: str\n    object: Literal[\"response\"] = \"response\"\n    created: int\n    model: str\n    output: List[ResponseItem]\n</code></pre>"},{"location":"api/#openresponses.models.MessageItem","title":"<code>openresponses.models.MessageItem</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a standard chat message.</p> Source code in <code>src/openresponses/models.py</code> <pre><code>class MessageItem(BaseModel):\n    \"\"\"Represents a standard chat message.\"\"\"\n    type: Literal[\"message\"] = \"message\"\n    role: Literal[\"user\", \"assistant\", \"system\"]\n    content: Union[str, List[InputText]] \n</code></pre>"},{"location":"api/#openresponses.models.ReasoningItem","title":"<code>openresponses.models.ReasoningItem</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a block of reasoning/thought process.</p> Source code in <code>src/openresponses/models.py</code> <pre><code>class ReasoningItem(BaseModel):\n    \"\"\"Represents a block of reasoning/thought process.\"\"\"\n    type: Literal[\"reasoning\"] = \"reasoning\"\n    content: Optional[str] = Field(default=None, description=\"Raw reasoning traces (Transparent)\")\n    summary: Optional[str] = Field(default=None, description=\"Sanitized summary\")\n    encrypted_content: Optional[str] = Field(default=None, description=\"Provider-secure reasoning\")\n</code></pre>"},{"location":"api/#openresponses.models.ToolCallItem","title":"<code>openresponses.models.ToolCallItem</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a tool call request.</p> Source code in <code>src/openresponses/models.py</code> <pre><code>class ToolCallItem(BaseModel):\n    \"\"\"Represents a tool call request.\"\"\"\n    type: Literal[\"tool_call\"] = \"tool_call\"\n    id: str\n    name: str\n    arguments: Dict[str, Any]\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>The <code>openresponses-python</code> repository includes several proxy implementations to adapt popular LLM providers to the Open Responses standard.</p> <p>All examples are located in the <code>examples/</code> directory.</p>"},{"location":"examples/#openrouter","title":"OpenRouter","text":"<p>Proxies requests to OpenRouter. Perfect for accessing models like DeepSeek R1, Claude 3.5, etc.</p> <p>Run:</p> <pre><code>make run-openrouter\n</code></pre> <p>Port: <code>8001</code> Env: <code>OPENROUTER_API_KEY</code></p>"},{"location":"examples/#openai","title":"OpenAI","text":"<p>Proxies requests to OpenAI.</p> <p>Run:</p> <pre><code>make run-openai\n</code></pre> <p>Port: <code>8002</code> Env: <code>OPENAI_API_KEY</code></p>"},{"location":"examples/#ollama-local","title":"Ollama (Local)","text":"<p>Proxies to a local Ollama instance running on <code>localhost:11434</code>.</p> <p>Run:</p> <pre><code>make run-ollama\n</code></pre> <p>Port: <code>8003</code></p>"},{"location":"examples/#lm-studio-local","title":"LM Studio (Local)","text":"<p>Proxies to local LM Studio running on <code>localhost:1234</code>.</p> <p>Run:</p> <pre><code>make run-lmstudio\n</code></pre> <p>Port: <code>8004</code></p>"},{"location":"examples/#huggingface-inference-api-tgi","title":"HuggingFace (Inference API / TGI)","text":"<p>Proxies to HuggingFace Inference Endpoints or TGI.</p> <p>Run:</p> <pre><code>make run-huggingface\n</code></pre> <p>Port: <code>8005</code> Env: <code>HF_API_KEY</code> (Optional), <code>HF_BASE_URL</code> (Defaults to public API)</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This guide will help you send your first request using <code>openresponses-python</code>.</p>"},{"location":"quickstart/#1-setup-a-provider","title":"1. Setup a Provider","text":"<p>The SDK connects to any Open Responses compliant API. For this example, we will use the OpenRouter Proxy included in the package examples.</p> <p>Ensure you have your API key set:</p> <pre><code>export OPENROUTER_API_KEY=sk-or-...\n</code></pre> <p>Run the example proxy in a separate terminal:</p> <pre><code>make run-openrouter\n# Server starts at http://localhost:8001\n</code></pre>"},{"location":"quickstart/#2-create-the-client-application","title":"2. Create the Client Application","text":"<p>Create a file <code>main.py</code>:</p> <pre><code>import asyncio\nfrom openresponses.client import AsyncOpenResponsesClient\n\nasync def main():\n    # Connect to the local provider we started\n    client = AsyncOpenResponsesClient(base_url=\"http://localhost:8001\")\n\n    print(\"Sending request...\")\n\n    # Send a request with streaming enabled\n    stream = await client.create(\n        model=\"deepseek/deepseek-r1\",\n        input=\"Why is the sky blue?\",\n        stream=True\n    )\n\n    print(\"\\nResponse:\")\n    async for event in stream:\n        if event.event == \"response.reasoning.delta\":\n             # Print reasoning (thinking) in grey or italic\n             print(f\"\\033[90m{event.data['delta']}\\033[0m\", end=\"\", flush=True)\n        elif event.event == \"response.text.delta\":\n             # Print final answer\n             print(event.data['delta'], end=\"\", flush=True)\n\n    print(\"\\n\\nDone!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"quickstart/#3-run-it","title":"3. Run It","text":"<pre><code>python main.py\n</code></pre> <p>You should see the models \"thinking\" process (if supported) followed by the answer.</p>"}]}